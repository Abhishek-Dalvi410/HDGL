{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPo4eXhDBQo9HuRdvBVpL2T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4MX-0I3YU-U9"},"outputs":[],"source":["import numpy as np\n","import scipy.sparse as sp\n","import torch\n","\n","\n","def accuracy(output, labels):\n","  preds = output.max(1)[1].type_as(labels)\n","  correct = preds.eq(labels).double()\n","  correct = correct.sum()\n","  return correct / len(labels)"]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","import math\n","\n","import torch\n","\n","from torch.nn.parameter import Parameter\n","from torch.nn.modules.module import Module\n","\n","\n","class GraphConvolution(Module):\n","    \"\"\"\n","    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n","    \"\"\"\n","\n","    def __init__(self, in_features, out_features, bias=True):\n","        super(GraphConvolution, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n","        if bias:\n","            self.bias = Parameter(torch.FloatTensor(out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        stdv = 1. / math.sqrt(self.weight.size(1))\n","        self.weight.data.uniform_(-stdv, stdv)\n","        if self.bias is not None:\n","            self.bias.data.uniform_(-stdv, stdv)\n","\n","    def forward(self, input, adj):\n","        support = torch.mm(input, self.weight)\n","        output = torch.spmm(adj, support)\n","        if self.bias is not None:\n","            return output + self.bias\n","        else:\n","            return output\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + ' (' \\\n","               + str(self.in_features) + ' -> ' \\\n","               + str(self.out_features) + ')'"],"metadata":{"id":"uXG5V9FaV5gP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class GCN(nn.Module):\n","    def __init__(self, nfeat, nhid, nclass, dropout):\n","        super(GCN, self).__init__()\n","\n","        self.gc1 = GraphConvolution(nfeat, nhid)\n","        self.gc2 = GraphConvolution(nhid, nclass)\n","        self.dropout = dropout\n","\n","    def forward(self, x, adj):\n","        x = F.relu(self.gc1(x, adj))\n","        x = F.dropout(x, self.dropout, training=self.training)\n","        x = self.gc2(x, adj)\n","        return F.log_softmax(x, dim=1)\n","\n"],"metadata":{"id":"qufHswkFV_x4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install dgl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_vh9LYZ6s-Qe","executionInfo":{"status":"ok","timestamp":1713574280519,"user_tz":240,"elapsed":50634,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"}},"outputId":"55cd9275-7dbd-467a-cd2e-8936329a6188"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting dgl\n","  Downloading dgl-2.1.0-cp310-cp310-manylinux1_x86_64.whl (8.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.25.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n","Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (0.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.2.2)\n","Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.2.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, dgl\n","Successfully installed dgl-2.1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"]}]},{"cell_type":"code","source":["def split_train_val_test_ids(labels, train_samples_per_class=20, val_samples_per_class=30):\n","    unique_labels = np.unique(labels)\n","\n","    train_ids = []\n","    val_ids = []\n","    test_ids = []\n","\n","    for label in unique_labels:\n","        # Get indices of samples with the current label\n","        label_indices = np.where(labels == label)[0]\n","\n","        # Shuffle the indices to randomize the samples\n","        np.random.shuffle(label_indices)\n","\n","        # Split the indices into train, val, and test sets\n","        train_indices = label_indices[:train_samples_per_class]\n","        val_indices = label_indices[train_samples_per_class:(train_samples_per_class + val_samples_per_class)]\n","        test_indices = label_indices[(train_samples_per_class + val_samples_per_class):]\n","\n","        train_ids.extend(train_indices)\n","        val_ids.extend(val_indices)\n","        test_ids.extend(test_indices)\n","\n","    return train_ids, val_ids, test_ids"],"metadata":{"id":"Bx-nHpg0MFuY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from dgl.data import DGLDataset\n","\n","class Blogcatalog(DGLDataset):\n","    def __init__(self):\n","        super().__init__(name=\"Blogcatalog\")\n","\n","    def process(self):\n","          print(\"Loading Blogcatalog Graph dataset....\")\n","          data = np.load('blogcatalog.npz', allow_pickle=True)\n","          labels = data['node_label']\n","          feat = data['node_attr']\n","          adj_matrix =  data['adj_matrix']\n","          feat = torch.tensor(feat.tolist().toarray()).float()\n","          labels = torch.tensor(labels)\n","          labels = labels.to(torch.int64)\n","          labels = labels - 1\n","          adj_matrix = adj_matrix.tolist().toarray()\n","          adj_matrix = adj_matrix + np.transpose(adj_matrix) + np.eye(adj_matrix.shape[0])\n","          print(adj_matrix)\n","          src, dst = np.nonzero(adj_matrix)\n","\n","          self.graph = dgl.graph(\n","            (src, dst), num_nodes=adj_matrix.shape[0]\n","            )\n","          self.graph.ndata[\"feat\"] = feat\n","          self.graph.ndata[\"label\"] = labels\n","          self.num_classes = len(np.unique(labels))\n","          print(\"Loading Done\")\n","\n","    def __getitem__(self, i):\n","        return self.graph\n","\n","    def __len__(self):\n","        return 1"],"metadata":{"id":"7tK7PeLwY2xH","executionInfo":{"status":"ok","timestamp":1713574286492,"user_tz":240,"elapsed":1380,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a7629b16-a6de-4359-89c0-85ab8b19adf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n"]},{"output_type":"stream","name":"stdout","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"]}]},{"cell_type":"code","source":["class DBLP(DGLDataset):\n","    def __init__(self):\n","        super().__init__(name=\"DBLP\")\n","\n","    def process(self):\n","          print(\"Loading DBLP Graph dataset....\")\n","          data = np.load('DBLP_BERT_graph_data.npz', allow_pickle=True)\n","          labels = data['labels']\n","          feat = data['feature_matrix']\n","          adj_matrix =  data['adj_mat']\n","          print(labels)\n","\n","          feat = torch.tensor(feat).float()\n","          labels = torch.tensor(labels)\n","          labels = labels.to(torch.int64)\n","          print(torch.unique(labels))\n","          adj_matrix = adj_matrix.tolist().toarray()\n","          adj_matrix = adj_matrix + np.transpose(adj_matrix) + np.eye(adj_matrix.shape[0])\n","          print(adj_matrix)\n","          src, dst = np.nonzero(adj_matrix)\n","\n","          self.graph = dgl.graph(\n","            (src, dst), num_nodes=adj_matrix.shape[0]\n","            )\n","          self.graph.ndata[\"feat\"] = feat\n","          self.graph.ndata[\"label\"] = labels\n","          self.num_classes = len(np.unique(labels))\n","          print(\"Loading Done\")\n","\n","    def __getitem__(self, i):\n","        return self.graph\n","\n","    def __len__(self):\n","        return 1"],"metadata":{"id":"Y8VknG877LoK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Change dataset_name in the below cell for running experiment on specific dataset."],"metadata":{"id":"-EqKCOW9P5IW"}},{"cell_type":"code","source":["import numpy as np\n","import time\n","import torch\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import dgl\n","from dgl.data import CoraGraphDataset, CiteseerGraphDataset, PubmedGraphDataset, CoauthorCSDataset, CoauthorPhysicsDataset\n","import torch.sparse as sp\n","import dgl.function as fn\n","\n","from dgl import AddSelfLoop\n","import dgl.sparse as dglsp\n","\n","\n","dataset_name = \"DBLP\" # change according\n","\n","if dataset_name == \"cora\":\n","  dataset = CoraGraphDataset(transform= AddSelfLoop())\n","\n","elif dataset_name == \"citeseer\":\n","  dataset = CiteseerGraphDataset(transform= AddSelfLoop())\n","\n","elif dataset_name == \"pubmed\":\n","  dataset = PubmedGraphDataset(transform= AddSelfLoop())\n","\n","elif dataset_name == \"CS\":\n","  dataset = CoauthorCSDataset(transform= AddSelfLoop())\n","\n","elif dataset_name == \"Physics\":\n","  dataset = CoauthorPhysicsDataset(transform= AddSelfLoop())\n","\n","elif dataset_name == \"Blogcatalog\":\n","  dataset = Blogcatalog()\n","\n","elif dataset_name == \"DBLP\":\n","  dataset = DBLP()\n","\n","\n","num_classes = dataset.num_classes\n","g = dataset[0]\n","degs = g.in_degrees().float()\n","norm = torch.pow(degs, -0.5)\n","norm[torch.isinf(norm)] = 0\n","g.ndata['norm'] = norm.unsqueeze(1)\n","g.apply_edges(fn.u_mul_v('norm', 'norm', 'normalized'))\n","\n","if dataset_name == \"cora\" or dataset_name == \"citeseer\" or dataset_name == \"pubmed\":\n","  train_mask = g.ndata['train_mask']\n","  val_mask = g.ndata['val_mask']\n","  test_mask = g.ndata['test_mask']\n","\n","elif dataset_name == \"CS\" or dataset_name == \"Physics\" or dataset_name == \"Blogcatalog\" or dataset_name == \"DBLP\":\n","  train_node_ids, val_node_ids, test_node_ids = split_train_val_test_ids(g.ndata['label'].numpy())\n","  train_mask = np.zeros(g.num_nodes(), dtype=bool)\n","  train_mask[train_node_ids] = True\n","\n","  val_mask = np.zeros(g.num_nodes(), dtype=bool)\n","  val_mask[val_node_ids] = True\n","\n","  test_mask = np.zeros(g.num_nodes(), dtype=bool)\n","  test_mask[test_node_ids] = True\n","\n","  train_mask = torch.from_numpy(train_mask)\n","  val_mask = torch.from_numpy(val_mask)\n","  test_mask = torch.from_numpy(test_mask)\n","\n","# get labels\n","labels = g.ndata['label']\n","features = g.ndata['feat']\n","\n","print(torch.unique(features))\n","print(torch.max(features))\n","print(torch.min(features))\n","\n","print(\"------\")\n","\n","\n","num_nodes = g.number_of_nodes()\n","\n","adj = g.adj()\n","\n","indices = adj.indices()\n","values = torch.squeeze(g.edata['normalized'])\n","shape = adj.shape\n","\n","adj = torch.sparse_coo_tensor(indices, values, shape)\n","\n","print(adj)\n","\n","\n","idx_train = torch.nonzero(train_mask).flatten()\n","idx_val = torch.nonzero(val_mask).flatten()\n","idx_test = torch.nonzero(test_mask).flatten()\n"],"metadata":{"id":"1glLeMQ10rX8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713575867312,"user_tz":240,"elapsed":8131,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"}},"outputId":"201d0797-c62e-416e-d17c-f0dbe2315a18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading DBLP Graph dataset....\n","[0 0 0 ... 3 3 3]\n","tensor([0, 1, 2, 3])\n","[[1. 2. 2. ... 0. 0. 0.]\n"," [2. 1. 0. ... 0. 0. 0.]\n"," [2. 0. 1. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 1. 0. 0.]\n"," [0. 0. 0. ... 0. 1. 0.]\n"," [0. 0. 0. ... 0. 0. 1.]]\n","Loading Done\n","tensor([-5.1318, -4.9828, -4.9130,  ...,  1.9756,  2.0021,  2.0364])\n","tensor(2.0364)\n","tensor(-5.1318)\n","------\n","tensor(indices=tensor([[    0,     0,     0,  ..., 17723, 17724, 17724],\n","                       [    0,     1,     2,  ..., 17723,  3434, 17724]]),\n","       values=tensor([0.3333, 0.2582, 0.2887,  ..., 0.5000, 0.1768, 0.5000]),\n","       size=(17725, 17725), nnz=123459, layout=torch.sparse_coo)\n"]}]},{"cell_type":"code","source":["def train(epoch, verbose):\n","    t = time.time()\n","    model.train()\n","    optimizer.zero_grad()\n","    output = model(features, adj)\n","    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n","    acc_train = accuracy(output[idx_train], labels[idx_train])\n","    loss_train.backward()\n","    optimizer.step()\n","\n","    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n","    acc_val = accuracy(output[idx_val], labels[idx_val])\n","\n","    if verbose:\n","      print('Epoch: {:04d}'.format(epoch+1),\n","            'loss_train: {:.4f}'.format(loss_train.item()),\n","            'acc_train: {:.4f}'.format(acc_train.item()),\n","            'loss_val: {:.4f}'.format(loss_val.item()),\n","            'acc_val: {:.4f}'.format(acc_val.item()),\n","            'time: {:.4f}s'.format(time.time() - t))\n","\n","    return loss_val.item()\n","\n","\n","def test():\n","    model.eval()\n","    output = model(features, adj)\n","    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n","    acc_test = accuracy(output[idx_test], labels[idx_test])\n","    print(\"Test set results:\",\n","          \"loss= {:.4f}\".format(loss_test.item()),\n","          \"accuracy= {:.4f}\".format(acc_test.item()))\n"],"metadata":{"id":"YEpdn7Mr0fSf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from itertools import product\n","from tqdm import tqdm\n","\n","# Define hyperparameter grid\n","epochs = 1000\n","hidden_list = [32, 64]\n","dropout = 0.5\n","min_delta = 0.001\n","lr_list = [0.01, 0.001]\n","weight_decay_list = [0.001, 0.0001, 5e-4]\n","\n","# Store results\n","results = []\n","\n","# Define a dictionary to map learning rates to patience values\n","lr_patience_dict = {0.01: 25, 0.001: 50}\n","\n","t_start = time.time()\n","\n","# Perform grid search\n","# for hidden, lr, weight_decay in product(hidden_list, lr_list, weight_decay_list):\n","for hidden, lr, weight_decay in tqdm(list(product(hidden_list, lr_list, weight_decay_list)), desc=\"Hyperparameter Grid Search\"):\n","\n","    # Create the GCN model with current hyperparameters\n","    model = GCN(nfeat=features.shape[1],\n","                nhid=hidden,\n","                nclass=labels.max().item() + 1,\n","                dropout=dropout)\n","\n","    optimizer = optim.Adam(model.parameters(),\n","                           lr=lr, weight_decay=weight_decay)\n","\n","    # Train the model\n","\n","    best_val_loss = float('inf')\n","    current_patience = 0\n","    patience = lr_patience_dict.get(lr, 10)  # Get patience from the dictionary or use a default value\n","\n","    for epoch in range(epochs):\n","        curr_val_loss = train(epoch, False)\n","\n","        # Early stopping check\n","        if best_val_loss - curr_val_loss > min_delta:\n","            best_val_loss = curr_val_loss\n","            current_patience = 0\n","        else:\n","            current_patience += 1\n","\n","        if current_patience > patience:\n","            # print(f'Early stopping at epoch {epoch}')\n","            break\n","\n","    # Store the results\n","    results.append({\n","        'hidden': hidden,\n","        'lr': lr,\n","        'weight_decay': weight_decay,\n","        'val_loss': best_val_loss,\n","    })\n","\n","\n","    print(\"Hidden:\", hidden,\n","        \"lr:\", lr,\n","        \"weight_decay:\", weight_decay,\n","        \"val_loss:\", best_val_loss)\n","\n","# Find the best set of hyperparameters\n","best_result = min(results, key=lambda x: x['val_loss'])\n","\n","\n","t_end = time.time()\n","print(\"------------------------\")\n","print(f\"Total Time Elapsed to Find Best Hyper-parameters: {t_end-t_start} seconds\")\n","print(\"------------------------\")\n","\n","# Print the best hyperparameters and test the model\n","print(\"Best Hyperparameters:\")\n","print(f\"Hidden: {best_result['hidden']}\")\n","print(f\"Learning Rate: {best_result['lr']}\")\n","print(f\"Weight Decay: {best_result['weight_decay']}\")\n","print(f\"Validation Loss: {best_result['val_loss']}\")\n","\n","print(\"------------------------\")\n","\n","\n","# Update the model with the best hyperparameters\n","model = GCN(nfeat=features.shape[1],\n","            nhid=best_result['hidden'],\n","            nclass=labels.max().item() + 1,\n","            dropout=dropout)\n","\n","optimizer = optim.Adam(model.parameters(),\n","                       lr=best_result['lr'], weight_decay=best_result['weight_decay'])\n","\n","\n","print(\"Now training with best Hyper-paramater settings\")\n","\n","best_val_loss = float('inf')\n","current_patience = 0\n","patience = lr_patience_dict.get(best_result['lr'], 10)  # Get patience from the dictionary or use a default value\n","# Train the model again with the best hyperparameters\n","for epoch in range(epochs):\n","  curr_val_loss = train(epoch, True)\n","\n","  # Early stopping check\n","  if best_val_loss - curr_val_loss > min_delta:\n","    best_val_loss = curr_val_loss\n","    current_patience = 0\n","  else:\n","        current_patience += 1\n","\n","  if current_patience > patience:\n","    print(f'Early stopping at epoch {epoch}')\n","    break\n","\n","# Test the final model\n","test()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BeIWqMZt-qES","executionInfo":{"status":"ok","timestamp":1713575980891,"user_tz":240,"elapsed":113580,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"}},"outputId":"b05b9ad9-484e-4322-d5cb-49b472e579f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Hyperparameter Grid Search:   8%|▊         | 1/12 [00:06<01:12,  6.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Hidden: 32 lr: 0.01 weight_decay: 0.001 val_loss: 0.7156115174293518\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter Grid Search:  17%|█▋        | 2/12 [00:11<00:56,  5.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Hidden: 32 lr: 0.01 weight_decay: 0.0001 val_loss: 0.8101913928985596\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter Grid Search:  25%|██▌       | 3/12 [00:15<00:45,  5.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Hidden: 32 lr: 0.01 weight_decay: 0.0005 val_loss: 0.8321588039398193\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter Grid Search:  33%|███▎      | 4/12 [00:28<01:04,  8.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Hidden: 32 lr: 0.001 weight_decay: 0.001 val_loss: 0.7362015247344971\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter Grid Search:  42%|████▏     | 5/12 [00:41<01:09,  9.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Hidden: 32 lr: 0.001 weight_decay: 0.0001 val_loss: 0.751530647277832\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter Grid Search:  50%|█████     | 6/12 [00:54<01:04, 10.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Hidden: 32 lr: 0.001 weight_decay: 0.0005 val_loss: 0.6944833397865295\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter Grid Search:  58%|█████▊    | 7/12 [01:00<00:46,  9.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Hidden: 64 lr: 0.01 weight_decay: 0.001 val_loss: 0.7851048111915588\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter Grid Search:  67%|██████▋   | 8/12 [01:04<00:30,  7.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Hidden: 64 lr: 0.01 weight_decay: 0.0001 val_loss: 0.8174694180488586\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter Grid Search:  75%|███████▌  | 9/12 [01:10<00:21,  7.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Hidden: 64 lr: 0.01 weight_decay: 0.0005 val_loss: 0.6818962097167969\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter Grid Search:  83%|████████▎ | 10/12 [01:23<00:17,  8.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Hidden: 64 lr: 0.001 weight_decay: 0.001 val_loss: 0.7323377132415771\n"]},{"output_type":"stream","name":"stderr","text":["\rHyperparameter Grid Search:  92%|█████████▏| 11/12 [01:33<00:09,  9.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Hidden: 64 lr: 0.001 weight_decay: 0.0001 val_loss: 0.7127971649169922\n"]},{"output_type":"stream","name":"stderr","text":["Hyperparameter Grid Search: 100%|██████████| 12/12 [01:44<00:00,  8.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Hidden: 64 lr: 0.001 weight_decay: 0.0005 val_loss: 0.6681440472602844\n","------------------------\n","Total Time Elapsed to Find Best Hyper-parameters: 104.66503047943115 seconds\n","------------------------\n","Best Hyperparameters:\n","Hidden: 64\n","Learning Rate: 0.001\n","Weight Decay: 0.0005\n","Validation Loss: 0.6681440472602844\n","------------------------\n","Now training with best Hyper-paramater settings\n","Epoch: 0001 loss_train: 1.7441 acc_train: 0.1875 loss_val: 1.8855 acc_val: 0.2167 time: 0.0884s\n","Epoch: 0002 loss_train: 1.4988 acc_train: 0.2625 loss_val: 1.4975 acc_val: 0.2750 time: 0.0882s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0003 loss_train: 1.2749 acc_train: 0.4875 loss_val: 1.4520 acc_val: 0.2917 time: 0.0885s\n","Epoch: 0004 loss_train: 1.2060 acc_train: 0.4750 loss_val: 1.4119 acc_val: 0.3583 time: 0.0894s\n","Epoch: 0005 loss_train: 1.1513 acc_train: 0.5250 loss_val: 1.3425 acc_val: 0.3667 time: 0.0922s\n","Epoch: 0006 loss_train: 1.0194 acc_train: 0.6000 loss_val: 1.2780 acc_val: 0.4250 time: 0.0884s\n","Epoch: 0007 loss_train: 1.0348 acc_train: 0.6375 loss_val: 1.1597 acc_val: 0.5333 time: 0.0889s\n","Epoch: 0008 loss_train: 0.9662 acc_train: 0.5875 loss_val: 1.1696 acc_val: 0.5083 time: 0.0857s\n","Epoch: 0009 loss_train: 0.9811 acc_train: 0.6750 loss_val: 1.1188 acc_val: 0.5083 time: 0.0884s\n","Epoch: 0010 loss_train: 0.9113 acc_train: 0.6750 loss_val: 1.0948 acc_val: 0.4917 time: 0.0895s\n","Epoch: 0011 loss_train: 0.8587 acc_train: 0.7125 loss_val: 1.0744 acc_val: 0.5500 time: 0.0858s\n","Epoch: 0012 loss_train: 0.7670 acc_train: 0.7875 loss_val: 1.1103 acc_val: 0.5583 time: 0.0854s\n","Epoch: 0013 loss_train: 0.7959 acc_train: 0.7125 loss_val: 1.0275 acc_val: 0.5667 time: 0.0882s\n","Epoch: 0014 loss_train: 0.8114 acc_train: 0.7000 loss_val: 1.0235 acc_val: 0.6000 time: 0.0854s\n","Epoch: 0015 loss_train: 0.7013 acc_train: 0.7625 loss_val: 0.9643 acc_val: 0.6083 time: 0.0880s\n","Epoch: 0016 loss_train: 0.6454 acc_train: 0.8250 loss_val: 0.9549 acc_val: 0.6667 time: 0.0906s\n","Epoch: 0017 loss_train: 0.6631 acc_train: 0.8000 loss_val: 0.9643 acc_val: 0.6250 time: 0.0886s\n","Epoch: 0018 loss_train: 0.6187 acc_train: 0.7750 loss_val: 0.9305 acc_val: 0.6583 time: 0.0872s\n","Epoch: 0019 loss_train: 0.5672 acc_train: 0.8250 loss_val: 0.9063 acc_val: 0.6917 time: 0.0885s\n","Epoch: 0020 loss_train: 0.5914 acc_train: 0.8500 loss_val: 0.8715 acc_val: 0.6500 time: 0.0869s\n","Epoch: 0021 loss_train: 0.5436 acc_train: 0.8500 loss_val: 0.8768 acc_val: 0.6917 time: 0.0889s\n","Epoch: 0022 loss_train: 0.5242 acc_train: 0.8250 loss_val: 0.9209 acc_val: 0.6000 time: 0.0900s\n","Epoch: 0023 loss_train: 0.5214 acc_train: 0.8500 loss_val: 0.7904 acc_val: 0.6667 time: 0.0886s\n","Epoch: 0024 loss_train: 0.4860 acc_train: 0.9250 loss_val: 0.8769 acc_val: 0.6917 time: 0.0898s\n","Epoch: 0025 loss_train: 0.5321 acc_train: 0.8125 loss_val: 0.9286 acc_val: 0.6500 time: 0.0871s\n","Epoch: 0026 loss_train: 0.4978 acc_train: 0.8125 loss_val: 0.8911 acc_val: 0.6500 time: 0.0851s\n","Epoch: 0027 loss_train: 0.4510 acc_train: 0.8625 loss_val: 0.9549 acc_val: 0.6417 time: 0.0899s\n","Epoch: 0028 loss_train: 0.4314 acc_train: 0.8000 loss_val: 0.8696 acc_val: 0.6250 time: 0.0872s\n","Epoch: 0029 loss_train: 0.4582 acc_train: 0.8375 loss_val: 0.8266 acc_val: 0.6917 time: 0.0880s\n","Epoch: 0030 loss_train: 0.4308 acc_train: 0.8625 loss_val: 0.8554 acc_val: 0.6250 time: 0.0870s\n","Epoch: 0031 loss_train: 0.4147 acc_train: 0.8625 loss_val: 0.8214 acc_val: 0.6583 time: 0.0868s\n","Epoch: 0032 loss_train: 0.3766 acc_train: 0.8875 loss_val: 0.7806 acc_val: 0.7167 time: 0.0901s\n","Epoch: 0033 loss_train: 0.4229 acc_train: 0.9000 loss_val: 0.8102 acc_val: 0.6750 time: 0.0852s\n","Epoch: 0034 loss_train: 0.4054 acc_train: 0.8625 loss_val: 0.8463 acc_val: 0.6667 time: 0.0876s\n","Epoch: 0035 loss_train: 0.3969 acc_train: 0.8875 loss_val: 0.8240 acc_val: 0.6917 time: 0.0863s\n","Epoch: 0036 loss_train: 0.3638 acc_train: 0.8875 loss_val: 0.8663 acc_val: 0.6417 time: 0.0855s\n","Epoch: 0037 loss_train: 0.3439 acc_train: 0.8750 loss_val: 0.7998 acc_val: 0.6833 time: 0.0883s\n","Epoch: 0038 loss_train: 0.3201 acc_train: 0.9250 loss_val: 0.7910 acc_val: 0.7333 time: 0.0856s\n","Epoch: 0039 loss_train: 0.3354 acc_train: 0.9250 loss_val: 0.7797 acc_val: 0.6917 time: 0.0969s\n","Epoch: 0040 loss_train: 0.3254 acc_train: 0.9125 loss_val: 0.8255 acc_val: 0.6500 time: 0.0883s\n","Epoch: 0041 loss_train: 0.3410 acc_train: 0.8875 loss_val: 0.7610 acc_val: 0.7083 time: 0.0855s\n","Epoch: 0042 loss_train: 0.3338 acc_train: 0.9375 loss_val: 0.8558 acc_val: 0.6583 time: 0.0936s\n","Epoch: 0043 loss_train: 0.3158 acc_train: 0.9125 loss_val: 0.7756 acc_val: 0.6833 time: 0.1002s\n","Epoch: 0044 loss_train: 0.3397 acc_train: 0.8875 loss_val: 0.8724 acc_val: 0.6667 time: 0.1020s\n","Epoch: 0045 loss_train: 0.3235 acc_train: 0.9250 loss_val: 0.8558 acc_val: 0.7000 time: 0.1031s\n","Epoch: 0046 loss_train: 0.3069 acc_train: 0.8750 loss_val: 0.8042 acc_val: 0.6667 time: 0.1006s\n","Epoch: 0047 loss_train: 0.2694 acc_train: 0.9375 loss_val: 0.8623 acc_val: 0.6833 time: 0.0914s\n","Epoch: 0048 loss_train: 0.2837 acc_train: 0.9625 loss_val: 0.7885 acc_val: 0.7083 time: 0.1052s\n","Epoch: 0049 loss_train: 0.2923 acc_train: 0.9000 loss_val: 0.7229 acc_val: 0.7083 time: 0.0938s\n","Epoch: 0050 loss_train: 0.2618 acc_train: 0.9500 loss_val: 0.8195 acc_val: 0.6583 time: 0.0918s\n","Epoch: 0051 loss_train: 0.2758 acc_train: 0.9375 loss_val: 0.7462 acc_val: 0.7000 time: 0.0869s\n","Epoch: 0052 loss_train: 0.2766 acc_train: 0.9375 loss_val: 0.8409 acc_val: 0.6500 time: 0.0904s\n","Epoch: 0053 loss_train: 0.2444 acc_train: 0.9250 loss_val: 0.7923 acc_val: 0.6583 time: 0.0878s\n","Epoch: 0054 loss_train: 0.2669 acc_train: 0.9500 loss_val: 0.7952 acc_val: 0.6667 time: 0.0849s\n","Epoch: 0055 loss_train: 0.2763 acc_train: 0.9125 loss_val: 0.7741 acc_val: 0.7500 time: 0.0881s\n","Epoch: 0056 loss_train: 0.2577 acc_train: 0.9625 loss_val: 0.7898 acc_val: 0.7083 time: 0.0886s\n","Epoch: 0057 loss_train: 0.2581 acc_train: 0.9375 loss_val: 0.8511 acc_val: 0.6750 time: 0.0884s\n","Epoch: 0058 loss_train: 0.2344 acc_train: 0.9375 loss_val: 0.7926 acc_val: 0.6333 time: 0.0862s\n","Epoch: 0059 loss_train: 0.2414 acc_train: 0.9375 loss_val: 0.8499 acc_val: 0.6583 time: 0.0862s\n","Epoch: 0060 loss_train: 0.2559 acc_train: 0.9375 loss_val: 0.8230 acc_val: 0.6583 time: 0.0864s\n","Epoch: 0061 loss_train: 0.2212 acc_train: 0.9375 loss_val: 0.7908 acc_val: 0.6667 time: 0.0887s\n","Epoch: 0062 loss_train: 0.2227 acc_train: 0.9500 loss_val: 0.8228 acc_val: 0.6917 time: 0.0875s\n","Epoch: 0063 loss_train: 0.2439 acc_train: 0.9375 loss_val: 0.8048 acc_val: 0.6833 time: 0.0875s\n","Epoch: 0064 loss_train: 0.2439 acc_train: 0.9500 loss_val: 0.8375 acc_val: 0.6750 time: 0.0887s\n","Epoch: 0065 loss_train: 0.2311 acc_train: 0.9500 loss_val: 0.8914 acc_val: 0.6417 time: 0.0866s\n","Epoch: 0066 loss_train: 0.2180 acc_train: 0.9375 loss_val: 0.8124 acc_val: 0.6917 time: 0.0874s\n","Epoch: 0067 loss_train: 0.2318 acc_train: 0.9500 loss_val: 0.8173 acc_val: 0.6583 time: 0.0879s\n","Epoch: 0068 loss_train: 0.1865 acc_train: 0.9500 loss_val: 0.8223 acc_val: 0.6833 time: 0.0872s\n","Epoch: 0069 loss_train: 0.2651 acc_train: 0.9375 loss_val: 0.8590 acc_val: 0.6583 time: 0.0841s\n","Epoch: 0070 loss_train: 0.1950 acc_train: 0.9500 loss_val: 0.7744 acc_val: 0.6917 time: 0.0865s\n","Epoch: 0071 loss_train: 0.2230 acc_train: 0.9250 loss_val: 0.8500 acc_val: 0.6667 time: 0.0880s\n","Epoch: 0072 loss_train: 0.1846 acc_train: 0.9625 loss_val: 0.7977 acc_val: 0.7333 time: 0.0984s\n","Epoch: 0073 loss_train: 0.2006 acc_train: 0.9625 loss_val: 0.8671 acc_val: 0.6583 time: 0.0888s\n","Epoch: 0074 loss_train: 0.2002 acc_train: 0.9500 loss_val: 0.8208 acc_val: 0.7167 time: 0.0892s\n","Epoch: 0075 loss_train: 0.2144 acc_train: 0.9500 loss_val: 0.9061 acc_val: 0.6833 time: 0.0859s\n","Epoch: 0076 loss_train: 0.1667 acc_train: 0.9750 loss_val: 0.8096 acc_val: 0.6417 time: 0.0923s\n","Epoch: 0077 loss_train: 0.1863 acc_train: 0.9750 loss_val: 0.8236 acc_val: 0.6833 time: 0.0883s\n","Epoch: 0078 loss_train: 0.1742 acc_train: 0.9500 loss_val: 0.8148 acc_val: 0.6917 time: 0.0920s\n","Epoch: 0079 loss_train: 0.1825 acc_train: 0.9625 loss_val: 0.7863 acc_val: 0.6833 time: 0.0948s\n","Epoch: 0080 loss_train: 0.1755 acc_train: 0.9750 loss_val: 0.9185 acc_val: 0.6750 time: 0.0860s\n","Epoch: 0081 loss_train: 0.1844 acc_train: 0.9625 loss_val: 0.7838 acc_val: 0.6583 time: 0.0880s\n","Epoch: 0082 loss_train: 0.1994 acc_train: 0.9500 loss_val: 0.7942 acc_val: 0.7000 time: 0.0872s\n","Epoch: 0083 loss_train: 0.1695 acc_train: 0.9625 loss_val: 0.8603 acc_val: 0.6917 time: 0.0867s\n","Epoch: 0084 loss_train: 0.1606 acc_train: 0.9625 loss_val: 0.8872 acc_val: 0.6750 time: 0.0872s\n","Epoch: 0085 loss_train: 0.1911 acc_train: 0.9500 loss_val: 0.8000 acc_val: 0.6833 time: 0.0845s\n","Epoch: 0086 loss_train: 0.1727 acc_train: 0.9625 loss_val: 0.7851 acc_val: 0.7083 time: 0.0886s\n","Epoch: 0087 loss_train: 0.1739 acc_train: 0.9500 loss_val: 0.8762 acc_val: 0.7000 time: 0.0887s\n","Epoch: 0088 loss_train: 0.1920 acc_train: 0.9625 loss_val: 0.8333 acc_val: 0.7000 time: 0.0869s\n","Epoch: 0089 loss_train: 0.1836 acc_train: 0.9625 loss_val: 0.8488 acc_val: 0.6917 time: 0.0862s\n","Epoch: 0090 loss_train: 0.1501 acc_train: 0.9625 loss_val: 0.8349 acc_val: 0.6583 time: 0.0882s\n","Epoch: 0091 loss_train: 0.1908 acc_train: 0.9250 loss_val: 0.8165 acc_val: 0.6750 time: 0.0868s\n","Epoch: 0092 loss_train: 0.1653 acc_train: 0.9750 loss_val: 0.8485 acc_val: 0.6750 time: 0.0845s\n","Epoch: 0093 loss_train: 0.1400 acc_train: 0.9750 loss_val: 0.8610 acc_val: 0.7000 time: 0.0851s\n","Epoch: 0094 loss_train: 0.1504 acc_train: 0.9750 loss_val: 0.8397 acc_val: 0.6833 time: 0.0807s\n","Epoch: 0095 loss_train: 0.1218 acc_train: 0.9875 loss_val: 0.7997 acc_val: 0.7083 time: 0.0836s\n","Epoch: 0096 loss_train: 0.1513 acc_train: 0.9625 loss_val: 0.8571 acc_val: 0.6500 time: 0.0835s\n","Epoch: 0097 loss_train: 0.1554 acc_train: 0.9625 loss_val: 0.8085 acc_val: 0.6667 time: 0.0864s\n","Epoch: 0098 loss_train: 0.1820 acc_train: 0.9500 loss_val: 0.7906 acc_val: 0.6750 time: 0.0825s\n","Epoch: 0099 loss_train: 0.1645 acc_train: 0.9500 loss_val: 0.8773 acc_val: 0.6750 time: 0.0855s\n","Epoch: 0100 loss_train: 0.1438 acc_train: 0.9625 loss_val: 0.8797 acc_val: 0.6833 time: 0.0836s\n","Early stopping at epoch 99\n","Test set results: loss= 0.7590 accuracy= 0.7212\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"LPF8bBB5H047"},"execution_count":null,"outputs":[]}]}