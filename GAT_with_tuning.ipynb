{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOXoL087JSZOeOdPSz/EIw6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This code is implemented using DGL with a PyTorch backend and is based on the official DGL implementation of Graph Attention Network (GAT) https://docs.dgl.ai/en/1.1.x/tutorials/models/1_gnn/9_gat.html. The tutorial at this link serves as the boilerplate code for this implementation, on top of which hyper-paramter tuning is added."],"metadata":{"id":"3dLUd-N2LbJt"}},{"cell_type":"code","source":["!pip install dgl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XDCVvfgSqjYK","executionInfo":{"status":"ok","timestamp":1713578077216,"user_tz":240,"elapsed":72097,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"}},"outputId":"60a8b8b7-1115-4a22-8271-9ce4946e03ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting dgl\n","  Downloading dgl-2.1.0-cp310-cp310-manylinux1_x86_64.whl (8.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.25.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n","Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (0.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.2.2)\n","Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.2.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata>=0.5.0->dgl)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, dgl\n","Successfully installed dgl-2.1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"]}]},{"cell_type":"code","source":["import dgl\n","import dgl.nn as dglnn\n","from dgl import AddSelfLoop\n","from dgl.data import CoraGraphDataset, CiteseerGraphDataset, PubmedGraphDataset, CoauthorCSDataset, CoauthorPhysicsDataset\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","import torch.nn as nn\n","import torch\n","import numpy as np"],"metadata":{"id":"TD8GUPrt0TAF","executionInfo":{"status":"ok","timestamp":1713578082373,"user_tz":240,"elapsed":5159,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5cf99e84-c919-48da-a317-68fa6edd82ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n"]},{"output_type":"stream","name":"stdout","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4MX-0I3YU-U9"},"outputs":[],"source":["class GAT(nn.Module):\n","    def __init__(self, in_size, hid_size, out_size, heads):\n","        super().__init__()\n","        self.gat_layers = nn.ModuleList()\n","        # two-layer GAT\n","        self.gat_layers.append(\n","            dglnn.GATConv(\n","                in_size,\n","                hid_size,\n","                heads[0],\n","                feat_drop=0.6,\n","                attn_drop=0.6,\n","                activation=F.elu,\n","            )\n","        )\n","        self.gat_layers.append(\n","            dglnn.GATConv(\n","                hid_size * heads[0],\n","                out_size,\n","                heads[1],\n","                feat_drop=0.6,\n","                attn_drop=0.6,\n","                activation=None,\n","            )\n","        )\n","\n","    def forward(self, g, inputs):\n","        h = inputs\n","        # print(g.adjacency_matrix())\n","        for i, layer in enumerate(self.gat_layers):\n","            h = layer(g, h)\n","            if i == 1:  # last layer\n","                h = h.mean(1)\n","            else:  # other layer(s)\n","                h = h.flatten(1)\n","        return h\n"]},{"cell_type":"code","source":["epochs = 1000\n","min_delta = 0.001\n","\n","def evaluate(g, features, labels, mask, model):\n","    model.eval()\n","    with torch.no_grad():\n","        logits = model(g, features)\n","        logits = logits[mask]\n","        labels = labels[mask]\n","        _, indices = torch.max(logits, dim=1)\n","        correct = torch.sum(indices == labels)\n","        loss_fcn = nn.CrossEntropyLoss()\n","        loss = loss_fcn(logits, labels)\n","        return loss.item(), correct.item() * 1.0 / len(labels)\n","\n","def train(g, features, labels, masks, model, learning_rate, weight_dec, patience, verbose):\n","    # define train/val samples, loss function and optimizer\n","    train_mask = masks[0]\n","    val_mask = masks[1]\n","    loss_fcn = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_dec)\n","\n","    best_val_loss = float('inf')\n","    current_patience = 0\n","    # training loop\n","    for epoch in range(epochs):\n","        model.train()\n","        logits = model(g, features)\n","        loss = loss_fcn(logits[train_mask], labels[train_mask])\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        loss_val = loss_fcn(logits[val_mask], labels[val_mask])\n","\n","        if verbose:\n","          _, acc_train = evaluate(g, features, labels, train_mask, model)\n","          _, acc_val = evaluate(g, features, labels, val_mask, model)\n","          print(\"Epoch {:05d} | Train Loss {:.4f} | Train Acc {:.4f} | Val Loss {:.4f} | Val Accuracy {:.4f} \".format(\n","              epoch, loss.item(), acc_train, loss_val.item(), acc_val))\n","\n","        curr_val_loss = loss_val.item()\n","\n","        # Early stopping check\n","        if best_val_loss - curr_val_loss > min_delta:\n","          best_val_loss = curr_val_loss\n","          current_patience = 0\n","        else:\n","          current_patience += 1\n","\n","        if current_patience > patience:\n","          if verbose:\n","            print(f'Early stopping at epoch {epoch}')\n","          break"],"metadata":{"id":"uXG5V9FaV5gP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from itertools import product\n","from tqdm import tqdm\n","\n","# Training settings\n","epochs = 1000\n","hidden_list = [8]\n","head_list = [4, 8]\n","min_delta = 0.001\n","lr_list = [0.01, 0.001]\n","weight_decay_list = [0.001, 0.0001, 5e-4]\n","\n","# Store results\n","results = []\n","\n","# Define a dictionary to map learning rates to patience values\n","lr_patience_dict = {0.01: 25, 0.001: 50}"],"metadata":{"id":"rtEwIarhVC0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def split_train_val_test_ids(labels, train_samples_per_class=20, val_samples_per_class=30):\n","    unique_labels = np.unique(labels)\n","\n","    train_ids = []\n","    val_ids = []\n","    test_ids = []\n","\n","    for label in unique_labels:\n","        # Get indices of samples with the current label\n","        label_indices = np.where(labels == label)[0]\n","\n","        # Shuffle the indices to randomize the samples\n","        np.random.shuffle(label_indices)\n","\n","        # Split the indices into train, val, and test sets\n","        train_indices = label_indices[:train_samples_per_class]\n","        val_indices = label_indices[train_samples_per_class:(train_samples_per_class + val_samples_per_class)]\n","        test_indices = label_indices[(train_samples_per_class + val_samples_per_class):]\n","\n","        train_ids.extend(train_indices)\n","        val_ids.extend(val_indices)\n","        test_ids.extend(test_indices)\n","\n","    return train_ids, val_ids, test_ids"],"metadata":{"id":"JT0p7ClmecX1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from dgl.data import DGLDataset\n","\n","class Blogcatalog(DGLDataset):\n","    def __init__(self):\n","        super().__init__(name=\"Blogcatalog\")\n","\n","    def process(self):\n","          print(\"Loading Blogcatalog Graph data...\")\n","          data = np.load('blogcatalog.npz', allow_pickle=True)\n","          labels = data['node_label']\n","          feat = data['node_attr']\n","          adj_matrix =  data['adj_matrix']\n","          feat = torch.tensor(feat.tolist().toarray()).float()\n","          labels = torch.tensor(labels)\n","          labels = labels.to(torch.int64)\n","          labels = labels - 1\n","          adj_matrix = adj_matrix.tolist().toarray()\n","          adj_matrix = adj_matrix + np.transpose(adj_matrix) + np.eye(adj_matrix.shape[0])\n","          print(adj_matrix)\n","          src, dst = np.nonzero(adj_matrix)\n","\n","          self.graph = dgl.graph(\n","            (src, dst), num_nodes=adj_matrix.shape[0]\n","            )\n","          self.graph.ndata[\"feat\"] = feat\n","          self.graph.ndata[\"label\"] = labels\n","          self.num_classes = len(np.unique(labels))\n","          print(\"Data loaded.\")\n","\n","    def __getitem__(self, i):\n","        return self.graph\n","\n","    def __len__(self):\n","        return 1\n","\n","class DBLP(DGLDataset):\n","    def __init__(self):\n","        super().__init__(name=\"DBLP\")\n","\n","    def process(self):\n","          print(\"Loading DBLP Graph data...\")\n","          data = np.load('DBLP_BERT_graph_data.npz', allow_pickle=True)\n","          labels = data['labels']\n","          feat = data['feature_matrix']\n","          adj_matrix =  data['adj_mat']\n","          feat = torch.tensor(feat).float()\n","          labels = torch.tensor(labels)\n","          labels = labels.to(torch.int64)\n","          adj_matrix = adj_matrix.tolist().toarray()\n","          adj_matrix = adj_matrix + np.transpose(adj_matrix) + np.eye(adj_matrix.shape[0])\n","          print(adj_matrix)\n","          src, dst = np.nonzero(adj_matrix)\n","\n","          self.graph = dgl.graph(\n","            (src, dst), num_nodes=adj_matrix.shape[0]\n","            )\n","          self.graph.ndata[\"feat\"] = feat\n","          self.graph.ndata[\"label\"] = labels\n","          self.num_classes = len(np.unique(labels))\n","          print(\"Data loaded.\")\n","\n","    def __getitem__(self, i):\n","        return self.graph\n","\n","    def __len__(self):\n","        return 1"],"metadata":{"id":"_XG2N516F66z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sGl8h_7Dave5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Change dataset_name in the below cell for running experiment on specific dataset."],"metadata":{"id":"03UZPiQPR1jK"}},{"cell_type":"code","source":["dataset_name = \"DBLP\" # change according\n","\n","if dataset_name == \"cora\":\n","  dataset = CoraGraphDataset(transform= AddSelfLoop())\n","\n","elif dataset_name == \"citeseer\":\n","  dataset = CiteseerGraphDataset(transform= AddSelfLoop())\n","\n","elif dataset_name == \"pubmed\":\n","  dataset = PubmedGraphDataset(transform= AddSelfLoop())\n","\n","elif dataset_name == \"CS\":\n","  dataset = CoauthorCSDataset(transform= AddSelfLoop())\n","\n","elif dataset_name == \"Physics\":\n","  dataset = CoauthorPhysicsDataset(transform= AddSelfLoop())\n","\n","elif dataset_name == \"Blogcatalog\":\n","  dataset = Blogcatalog()\n","\n","elif dataset_name == \"DBLP\":\n","  dataset = DBLP()\n","\n","else:\n","  raise NotImplementedError\n","\n","num_classes = dataset.num_classes\n","g = dataset[0]\n","# get labels\n","labels = g.ndata['label']\n","features = g.ndata['feat']\n","\n","print(labels.dtype)\n","print(features.dtype)\n","\n","if dataset_name == \"cora\" or dataset_name == \"citeseer\" or dataset_name == \"pubmed\":\n","  masks = g.ndata[\"train_mask\"], g.ndata[\"val_mask\"], g.ndata[\"test_mask\"]\n","\n","elif dataset_name == \"CS\" or dataset_name == \"Physics\" or dataset_name == \"Blogcatalog\" or dataset_name == \"DBLP\":\n","  train_node_ids, val_node_ids, test_node_ids = split_train_val_test_ids(g.ndata['label'].numpy())\n","  train_mask = np.zeros(g.num_nodes(), dtype=bool)\n","  train_mask[train_node_ids] = True\n","\n","  val_mask = np.zeros(g.num_nodes(), dtype=bool)\n","  val_mask[val_node_ids] = True\n","\n","  test_mask = np.zeros(g.num_nodes(), dtype=bool)\n","  test_mask[test_node_ids] = True\n","\n","  train_mask = torch.from_numpy(train_mask)\n","  val_mask = torch.from_numpy(val_mask)\n","  test_mask = torch.from_numpy(test_mask)\n","\n","  masks = train_mask, val_mask, test_mask\n","\n","# create GAT model\n","in_size = features.shape[1]\n","out_size = dataset.num_classes\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"olRJUkbGd9PL","executionInfo":{"status":"ok","timestamp":1713578307160,"user_tz":240,"elapsed":14724,"user":{"displayName":"Abhishek Dalvi","userId":"01466421205583090743"}},"outputId":"6391a50e-c138-4e19-94eb-771a401bc96b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading DBLP Graph data...\n","[[1. 2. 2. ... 0. 0. 0.]\n"," [2. 1. 0. ... 0. 0. 0.]\n"," [2. 0. 1. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 1. 0. 0.]\n"," [0. 0. 0. ... 0. 1. 0.]\n"," [0. 0. 0. ... 0. 0. 1.]]\n","Data loaded.\n","torch.int64\n","torch.float32\n"]}]},{"cell_type":"code","source":["\n","t_start = time.time()\n","# Perform grid search\n","# for hidden, lr, weight_decay in product(hidden_list, lr_list, weight_decay_list):\n","for hidden, lr, weight_decay, num_heads in tqdm(list(product(hidden_list, lr_list, weight_decay_list, head_list)), desc=\"Hyperparameter Grid Search\"):\n","  model = GAT(in_size, hidden, out_size, heads=[num_heads, 1])\n","  train(g, features, labels, masks, model, lr, weight_decay, lr_patience_dict[lr], False)\n","  val_loss, val_acc = evaluate(g, features, labels, masks[1], model)\n","\n","  results.append({\n","        'hidden': hidden,\n","        'heads': num_heads,\n","        'lr': lr,\n","        'weight_decay': weight_decay,\n","        'val_loss': val_loss,\n","    })\n","\n","  print(\"Hidden:\", hidden,\n","        \"heads:\", num_heads,\n","        \"lr:\", lr,\n","        \"weight_decay:\", weight_decay,\n","        \"val_loss:\", val_loss)\n","\n","\n","# Find the best set of hyperparameters\n","best_result = min(results, key=lambda x: x['val_loss'])\n","\n","\n","t_end = time.time()\n","print(\"------------------------\")\n","print(f\"Total Time Elapsed to Find Best Hyper-parameters: {t_end-t_start} seconds\")\n","print(\"------------------------\")\n","\n","# Print the best hyperparameters and test the model\n","print(\"Best Hyperparameters:\")\n","print(f\"Hidden: {best_result['hidden']}\")\n","print(f\"Heads: {best_result['heads']}\")\n","print(f\"Learning Rate: {best_result['lr']}\")\n","print(f\"Weight Decay: {best_result['weight_decay']}\")\n","print(f\"Validation Loss: {best_result['val_loss']}\")\n","\n","print(\"------------------------\")\n","\n","\n","print(\"Now training with best Hyper-paramater settings\")\n","\n","hidden = best_result['hidden']\n","lr = best_result['lr']\n","weight_decay = best_result['weight_decay']\n","num_heads = best_result['heads']\n","\n","model = GAT(in_size, hidden, out_size, heads=[num_heads, 1])\n","train(g, features, labels, masks, model, lr, weight_decay, lr_patience_dict[lr], True)\n","\n","# test the model\n","print(\"Testing...\")\n","_, acc = evaluate(g, features, labels, masks[2], model)\n","print(\"Test accuracy {:.4f}\".format(acc))\n"],"metadata":{"id":"k88P8gEXRxTQ"},"execution_count":null,"outputs":[]}]}